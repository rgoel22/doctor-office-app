name: Build Images and Deploy to AWS EKS Cluster

on:
  push:
    branches:
      - 'deploy/**'
      - main

env:
  ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
  AWS_REGION: ${{ vars.AWS_REGION }}
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME }}
  IMAGE_TAG: ${{ github.sha }} #latest
  CI_RUNNER_ROLE: ${{ secrets.CI_RUNNER_ROLE }}

jobs:
  check-prerequisites:
    runs-on: ubuntu-latest
    steps:
      - name: "Check/Install Prerequisites"
        run: |
          echo "Checking all the runtime prerequisites" 
          aws --version
          kubectl version --client
          helm version
        continue-on-error: false

  build:
    runs-on: ubuntu-latest
    needs: check-prerequisites
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4.2.2

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ vars.AWS_REGION }}

    - name: Login to ECR
      run: |
        aws sts get-caller-identity
        aws ecr get-login-password --region $AWS_REGION | \
        docker login --username AWS --password-stdin $ECR_REGISTRY

    - name: Build and Push Frontend Microservice Image to ECR
      run: |
        docker build -t frontend:$IMAGE_TAG ./doctor-office-frontend
        docker tag frontend:$IMAGE_TAG $ECR_REGISTRY/docktor-app/doctor-office-frontend:$IMAGE_TAG
        docker push $ECR_REGISTRY/docktor-app/doctor-office-frontend:$IMAGE_TAG

    - name: Build and Push Backend Microservice Image to ECR
      run: |
        docker build -t backend:$IMAGE_TAG ./doctor-office-backend
        docker tag backend:$IMAGE_TAG $ECR_REGISTRY/docktor-app/doctor-office-backend:$IMAGE_TAG
        docker push $ECR_REGISTRY/docktor-app/doctor-office-backend:$IMAGE_TAG

  deploy-eks-cluster:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4.2.2

      - name: Install eksctl
        run: |  
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Version check - eksctl
        run: |
          eksctl version 
        continue-on-error: false

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Deploy EKS Cluster
        id: eks_cluster_setup
        run: |    
          if ! eksctl get cluster -n $CLUSTER_NAME --region $AWS_REGION; then 
            echo "Verify eksctl installation.."
            eksctl version
            echo "Creating EKS Cluster...."
            eksctl create cluster -f ./aws_infra/eks_cluster_setup.yaml
            echo "::set-output name=cluster_exists::false"
            
          else
            echo "EKS Cluster $CLUSTER_NAME already running in $AWS_REGION...."
            eksctl get cluster -n $CLUSTER_NAME --region $AWS_REGION
            eksctl create iamidentitymapping --cluster $CLUSTER_NAME --arn $CI_RUNNER_ROLE --group system:masters --username admin
            echo "::set-output name=cluster_exists::true"
          fi
        continue-on-error: false

      - name: Install EKS Cluster Add-ons
        if: steps.eks_cluster_setup.outputs.cluster_exists == 'false'
        run: |
          echo "Installing EKS Cluster Add-ons"
          echo "Installing Application Load Balancer (ALB) Add-on....."
          export CLUSTER_VPC=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update eks
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=$CLUSTER_NAME \
            --set serviceAccount.create=false \
            --set region=${AWS_REGION} \
            --set vpcId=${CLUSTER_VPC} \
            --set serviceAccount.name=aws-load-balancer-controller

          echo "Installing EKS Cloud Observability/Container Insights Add-on....."
          eksctl create iamserviceaccount \
            --name cloudwatch-agent \
            --namespace amazon-cloudwatch --cluster $CLUSTER_NAME \
            --role-name aws-cloudwatch-agent \
            --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \
            --role-only \
            --approve

          aws eks create-addon \
            --addon-name amazon-cloudwatch-observability \
            --cluster-name docktor-app-cluster \
            --service-account-role-arn arn:aws:iam::619715105204:role/aws-cloudwatch-agent
        continue-on-error: false

  prod-k8s-deploy:
    runs-on: ubuntu-latest
    needs: deploy-eks-cluster
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4.2.2

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Deploy MongoDB StatefulSets
        run: | 
          echo "Setting up MongoDB Primary and Secondary replicaSets..."
          aws sts get-caller-identity
          aws eks --region $AWS_REGION update-kubeconfig --name $CLUSTER_NAME
          kubectl apply -f ./k8s/statefulset-mongo/storage-class.yaml
          sleep 5
          kubectl apply -f ./k8s/statefulset-mongo/mongo-statefulset.yaml
          sleep 60
          kubectl apply -f ./k8s/statefulset-mongo/mongo-service.yaml
          sleep 10
          
          echo "Check for ReplicaSets status..."
          RS_STATUS=$(kubectl exec mongo-0 -- mongosh --quiet --eval 'rs.status().ok')
          if [ "$RS_STATUS" != "1" ]; then
            kubectl exec mongo-0 -- mongosh --eval 'rs.initiate({_id: "rs0",members: [{_id: 0, host: "mongo-0.mongo.default.svc.cluster.local:27017" },{_id: 1, host: "mongo-1.mongo.default.svc.cluster.local:27017" },{_id: 2, host: "mongo-2.mongo.default.svc.cluster.local:27017" },]});'
          else
            echo "ReplicaSet already configured!"
          fi
          kubectl get all

      - name: Deploy k8s deployment yaml files
        run: |    
          aws sts get-caller-identity
          aws eks --region $AWS_REGION update-kubeconfig --name $CLUSTER_NAME
          echo "Applying Kubernetes configuration..."
          kubectl apply -f ./k8s/backend-deployment.yaml
          kubectl apply -f ./k8s/frontend-deployment.yaml
          kubectl apply -f ./k8s/ingress-controller.yaml
          kubectl apply -f ./k8s/metrics-server-components.yaml
          kubectl apply -f ./k8s/backend-hpa.yaml
          kubectl apply -f ./k8s/frontend-hpa.yaml
        continue-on-error: false

      - name: Check Deployment Status
        run: |
          echo "Verifying deployment status...."
          kubectl get all
          sleep 10
          kubectl get ingress -o json

      - name: Application Load Balancer URL
        run: | 
          echo "Frontend website will be available at the following URL:"
          APP_URL=$(kubectl get ingress -o json | jq -r .items[].status.loadBalancer.ingress[0].hostname)
          echo "http://$APP_URL"
  
  
